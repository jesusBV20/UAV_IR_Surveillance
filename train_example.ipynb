{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notebook muy sencillito para mostrar cómo hemos construido y entrenado a la red."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORTANTE: Instalar la base de datos siguiendo las instrucciones del README\n",
    "# e indicar en estas dos variables los direcctorios!!\n",
    "TRAIN_DATA_PATH = \"TrainReal\"\n",
    "TEST_DATA_PATH = \"TestReal\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Si se ejecuta desde colab, descomentar esta celda para descargar y\n",
    "# descomprimir los datos de forma automática.\n",
    "\n",
    "# from IPython.display import clear_output\n",
    "\n",
    "# !wget \"https://lilablobssc.blob.core.windows.net/conservationdrones/v01/conservation_drones_train_real.zip\"\n",
    "# !unzip conservation_drones_train_real.zip \n",
    "# clear_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pylab as plt\n",
    "\n",
    "from torchvision.models.detection import fasterrcnn_resnet50_fpn, FasterRCNN_ResNet50_FPN_Weights\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "\n",
    "WEIGHTS = FasterRCNN_ResNet50_FPN_Weights.DEFAULT\n",
    "\n",
    "def get_tunned_model(num_classes):\n",
    "  # Cargamos el modelo pre-entrenado\n",
    "  model = fasterrcnn_resnet50_fpn(weights=WEIGHTS)\n",
    "\n",
    "  # Reemplazamos el clasificador de la red\n",
    "  in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "  model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
    "  return model\n",
    "\n",
    "def get_transform():\n",
    "  return WEIGHTS.transforms()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comprobamos que podemos inferir sobre la red sin entrenar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from UAVIR_tools import BIRDSAIDataset, imgBoxes\n",
    "\n",
    "# Testeamos el tunned_model no entrenado\n",
    "transforms = get_transform()\n",
    "\n",
    "dataset = BIRDSAIDataset(TRAIN_DATA_PATH)\n",
    "img, _ = dataset[0]\n",
    "images = [transforms(d) for d in [img]]\n",
    "\n",
    "model = get_tunned_model(3)\n",
    "model_eval = model.eval()\n",
    "outputs = model_eval(images)\n",
    "\n",
    "# Mostramos el resultado\n",
    "img_box = imgBoxes(img, outputs[0][\"boxes\"], outputs[0][\"labels\"])\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot()\n",
    "ax.axis('off')\n",
    "\n",
    "ax.imshow(img_box)\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entrenamos el modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "\n",
    "from UAVIR_tools.detection.engine import train_one_epoch, evaluate\n",
    "import UAVIR_tools.detection.utils as utils\n",
    "\n",
    "# ------- MAIN --------- #\n",
    "# train on the GPU or on the CPU, if a GPU is not available\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "print(\"Device: {}\\n\".format(device))\n",
    "\n",
    "# our dataset has two classes only - background and person\n",
    "num_classes = 3 # 2 human, 1 animal, 0 background\n",
    "\n",
    "# use our dataset and defined transformations\n",
    "dataset = BIRDSAIDataset(TRAIN_DATA_PATH, get_transform(), 4000)\n",
    "dataset_test = BIRDSAIDataset(TEST_DATA_PATH, get_transform())\n",
    "\n",
    "# split the dataset in train and test set\n",
    "# indices = torch.randperm(len(dataset)).tolist()\n",
    "# dataset = torch.utils.data.Subset(dataset, indices[:-50])\n",
    "# dataset_test = torch.utils.data.Subset(dataset_test, indices[-50:])\n",
    "\n",
    "# define training and validation data loaders\n",
    "data_loader = torch.utils.data.DataLoader(\n",
    "  dataset, batch_size=10, shuffle=True, num_workers=4,\n",
    "  collate_fn=utils.collate_fn)\n",
    "\n",
    "data_loader_test = torch.utils.data.DataLoader(\n",
    "  dataset_test, batch_size=1, shuffle=False, num_workers=4,\n",
    "  collate_fn=utils.collate_fn)\n",
    "\n",
    "# get the model using our helper function\n",
    "model = get_tunned_model(num_classes) \n",
    "\n",
    "# move model to the right device\n",
    "model.to(device)\n",
    "\n",
    "# construct an optimizer\n",
    "params = [p for p in model.parameters() if p.requires_grad]\n",
    "optimizer = torch.optim.SGD(params, lr=0.005,\n",
    "                            momentum=0.9, weight_decay=0.0005)\n",
    "# and a learning rate scheduler\n",
    "lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer,\n",
    "                                                step_size=3,\n",
    "                                                gamma=0.1)\n",
    "\n",
    "# let's train it for 10 epochs\n",
    "num_epochs = 3\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "  # train for one epoch, printing every 10 iterations\n",
    "  train_one_epoch(model, optimizer, data_loader, device, epoch, print_freq=10)\n",
    "  # update the learning rate\n",
    "  lr_scheduler.step()\n",
    "  # evaluate on the test dataset\n",
    "  evaluate(model, data_loader_test, device=device)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comprobamos resultados realizando alguna inferencia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms as T\n",
    "\n",
    "transforms = get_transform()\n",
    "\n",
    "# Seleccionamos el vídeo y el frame\n",
    "movie_id = 21\n",
    "frame = 200\n",
    "\n",
    "frame_init, frame_end, n_frames = dataset.getMovieBoxInfo(movie_id)\n",
    "\n",
    "# Recogemos la img del dataset\n",
    "img, _ = dataset[frame_init + frame]\n",
    "images = [img.to(device)]\n",
    "\n",
    "# Inferimos\n",
    "model_eval = model.eval()\n",
    "outputs = model_eval(images)\n",
    "\n",
    "# Mostramos el resultado\n",
    "FIGSIZE = [16, 9]\n",
    "RES = 1280 # 720p\n",
    "\n",
    "transform_toImg = T.ToPILImage()\n",
    "img = transform_toImg(img)\n",
    "img_box = imgBoxes(img, outputs[0][\"boxes\"], outputs[0][\"labels\"])\n",
    "\n",
    "fig = plt.figure(figsize=FIGSIZE, dpi=RES/FIGSIZE[0])\n",
    "ax = fig.add_subplot()\n",
    "ax.axis('off')\n",
    "\n",
    "ax.imshow(img_box)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.3"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "081a26d6bbdb3ff7b6d982cd5633fd08a6c6252988ae80250aad25c8aed50509"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
