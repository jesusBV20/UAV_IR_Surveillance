{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "###################################################\n",
    "# IMPORTANTE: Instalar la base de datos siguiendo #\n",
    "# las instrucciones del README e indicar en estas #\n",
    "# variables los direcctorios!!                    #\n",
    "TRAIN_REAL_DATA_PATH = \"D:\\Datos\\TrainReal\"\n",
    "TRAIN_SIM_DATA_PATH  = \"D:\\Datos\\TrainSimulation\"\n",
    "TEST_DATA_PATH       = \"D:\\Datos\\TestReal\"\n",
    "###################################################\n",
    "\n",
    "OUTPUT_MODEL_PATH = \"model\"\n",
    "OUTPUT_INFERENCE_PATH = \"output\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inicialización (COLAB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'UAV_IR_Surveillance'...\n",
      "remote: Enumerating objects: 198, done.\u001b[K\n",
      "remote: Counting objects: 100% (198/198), done.\u001b[K\n",
      "remote: Compressing objects: 100% (153/153), done.\u001b[K\n",
      "remote: Total 198 (delta 88), reused 140 (delta 40), pack-reused 0\u001b[K\n",
      "Receiving objects: 100% (198/198), 6.46 MiB | 15.59 MiB/s, done.\n",
      "Resolving deltas: 100% (88/88), done.\n"
     ]
    }
   ],
   "source": [
    "# #####################################################\n",
    "# # Si se ejecuta desde colab, descomentar esta celda #\n",
    "# #####################################################\n",
    "\n",
    "# ## Para descargar el proyecto e importar todas las dependencias\n",
    "# !git clone https://github.com/jesusBV20/UAV_IR_Surveillance.git\n",
    "# !mv UAV_IR_Surveillance/* . \n",
    "# !rm -r UAV_IR_Surveillance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processor\t: 0\n",
      "vendor_id\t: GenuineIntel\n",
      "cpu family\t: 6\n",
      "model\t\t: 79\n",
      "model name\t: Intel(R) Xeon(R) CPU @ 2.20GHz\n",
      "stepping\t: 0\n",
      "microcode\t: 0xffffffff\n",
      "cpu MHz\t\t: 2199.998\n",
      "cache size\t: 56320 KB\n",
      "physical id\t: 0\n",
      "siblings\t: 2\n",
      "core id\t\t: 0\n",
      "cpu cores\t: 1\n",
      "apicid\t\t: 0\n",
      "initial apicid\t: 0\n",
      "fpu\t\t: yes\n",
      "fpu_exception\t: yes\n",
      "cpuid level\t: 13\n",
      "wp\t\t: yes\n",
      "flags\t\t: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc rep_good nopl xtopology nonstop_tsc cpuid tsc_known_freq pni pclmulqdq ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch invpcid_single ssbd ibrs ibpb stibp fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm rdseed adx smap xsaveopt arat md_clear arch_capabilities\n",
      "bugs\t\t: cpu_meltdown spectre_v1 spectre_v2 spec_store_bypass l1tf mds swapgs taa mmio_stale_data retbleed\n",
      "bogomips\t: 4399.99\n",
      "clflush size\t: 64\n",
      "cache_alignment\t: 64\n",
      "address sizes\t: 46 bits physical, 48 bits virtual\n",
      "power management:\n",
      "\n",
      "processor\t: 1\n",
      "vendor_id\t: GenuineIntel\n",
      "cpu family\t: 6\n",
      "model\t\t: 79\n",
      "model name\t: Intel(R) Xeon(R) CPU @ 2.20GHz\n",
      "stepping\t: 0\n",
      "microcode\t: 0xffffffff\n",
      "cpu MHz\t\t: 2199.998\n",
      "cache size\t: 56320 KB\n",
      "physical id\t: 0\n",
      "siblings\t: 2\n",
      "core id\t\t: 0\n",
      "cpu cores\t: 1\n",
      "apicid\t\t: 1\n",
      "initial apicid\t: 1\n",
      "fpu\t\t: yes\n",
      "fpu_exception\t: yes\n",
      "cpuid level\t: 13\n",
      "wp\t\t: yes\n",
      "flags\t\t: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc rep_good nopl xtopology nonstop_tsc cpuid tsc_known_freq pni pclmulqdq ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch invpcid_single ssbd ibrs ibpb stibp fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm rdseed adx smap xsaveopt arat md_clear arch_capabilities\n",
      "bugs\t\t: cpu_meltdown spectre_v1 spectre_v2 spec_store_bypass l1tf mds swapgs taa mmio_stale_data retbleed\n",
      "bogomips\t: 4399.99\n",
      "clflush size\t: 64\n",
      "cache_alignment\t: 64\n",
      "address sizes\t: 46 bits physical, 48 bits virtual\n",
      "power management:\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# #####################################################\n",
    "# # Si se ejecuta desde colab, descomentar esta celda #\n",
    "# #####################################################\n",
    "\n",
    "# ## ¿Qué CPU nos ha asignado google?\n",
    "# !cat /proc/cpuinfo | grep \"model name\"\n",
    "\n",
    "# ## ¿Qué GPU nos ha asignado google?\n",
    "# !nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #####################################################\n",
    "# # Si se ejecuta desde colab, descomentar esta celda #\n",
    "# #####################################################\n",
    "\n",
    "# ## Para descargar y descomprimir los datos de forma automática --\n",
    "# from IPython.display import clear_output\n",
    "\n",
    "# # TrainReal\n",
    "# !wget \"https://storage.googleapis.com/public-datasets-lila/conservationdrones/v01/conservation_drones_train_real.zip\"\n",
    "# !unzip conservation_drones_train_real.zip\n",
    "# clear_output()\n",
    "\n",
    "# # TestReal\n",
    "# !wget \"https://storage.googleapis.com/public-datasets-lila/conservationdrones/v01/conservation_drones_test_real.zip\"\n",
    "# !unzip conservation_drones_test_real.zip\n",
    "# clear_output()\n",
    "\n",
    "# # print(\"Done!\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cargamos librerías"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n",
      "\n",
      "¡El directorio 'output' ya existe!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pylab as plt\n",
    "\n",
    "import torch\n",
    "import torchvision.transforms as T\n",
    "from torchvision.models.detection import fasterrcnn_resnet50_fpn\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "from UAVIR_tools import BIRDSAIDataset, imgBoxes, createDir\n",
    "\n",
    "# Dimensiones de las figuras\n",
    "FIGSIZE = [16, 9]\n",
    "#RES = 1920 # Full HD\n",
    "RES = 2560 # 2k\n",
    "\n",
    "# Si se encuentra disponible, seleccionamos GPU como dispositivo para entrenar\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "print(\"Device: {}\\n\".format(device))\n",
    "\n",
    "\"\"\"\n",
    "Función que toma Faster-RCNN y genera un nuevo modelo\n",
    "aplicando un fine-tunning en el clasificador, adaptándolo al número\n",
    "de clases, que debe de ser indicado en 'num_classes'. \n",
    "\"\"\"\n",
    "def get_tunned_model(num_classes):\n",
    "    # Cargamos el modelo\n",
    "    model = fasterrcnn_resnet50_fpn()\n",
    "\n",
    "    # Reemplazamos el clasificador de la red\n",
    "    in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
    "    return model\n",
    "\n",
    "\"\"\"\n",
    "Función que nos retorna las transformaciones necesarias para codificar las\n",
    "imágenes (en formato PIL), de tal forma que sean interpretables por la red.\n",
    "\"\"\"\n",
    "def get_transform():\n",
    "    transforms = []\n",
    "    transforms.append(T.PILToTensor())\n",
    "    transforms.append(T.ConvertImageDtype(torch.float))\n",
    "    return T.Compose(transforms) # __call__(<PIL image>)\n",
    "\n",
    "\"\"\"\n",
    "Función para calcular el tamaño de un modelo. Es bastante simple,\n",
    "únicamente se fija en lo que ocupa el archivo '.pt'.\n",
    "\"\"\"\n",
    "def print_model_size(mdl):\n",
    "    torch.save(mdl.state_dict(), \"tmp.pt\")\n",
    "    print(\"%.2f MB\" %(os.path.getsize(\"tmp.pt\")/1e6))\n",
    "    os.remove('tmp.pt')\n",
    "\n",
    "# Transformaciones necesarias para que la entrada pueda\n",
    "# ser interpretada por el modelo\n",
    "transforms = get_transform()\n",
    "\n",
    "# Creamos el directorio donde vamos a guardar la salida\n",
    "createDir(OUTPUT_INFERENCE_PATH)\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Cuantizamos el modelo**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "################################\n",
    "# Cargamos el modelo entrenado #\n",
    "################################\n",
    "num_classes = 3\n",
    "model = get_tunned_model(num_classes)\n",
    "\n",
    "model.to(torch.device('cpu'))\n",
    "model.load_state_dict(torch.load(os.path.join(OUTPUT_MODEL_PATH, \"model_AUVIR_v0.3.pt\")))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cuantizamos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\rema0\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\ao\\quantization\\observer.py:214: UserWarning: Please use quant_min and quant_max to specify the range for observers.                     reduce_range will be deprecated in a future release of PyTorch.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "165.72 MB\n",
      "124.00 MB\n"
     ]
    }
   ],
   "source": [
    "# Inicualizamos el QuantWrappet para añadir funciones de\n",
    "# cuantización y de-cuantización\n",
    "quantized_model = torch.quantization.QuantWrapper(model)\n",
    "\n",
    "# Cuantizamos utilizando un backend concreto\n",
    "backend = \"fbgemm\"\n",
    "model.qconfig = torch.quantization.get_default_qconfig(backend)\n",
    "torch.backends.quantized.engine = backend\n",
    "\n",
    "model_static_quantized = torch.quantization.quantize_dynamic(quantized_model)\n",
    "\n",
    "# Mostramos el tamaño de ambos modelos, el base y el cuantizado\n",
    "print_model_size(model)\n",
    "print_model_size(model_static_quantized)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generamos un clip infiriendo con el modelo cuantizado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cpu\n",
      "\n",
      "Generando imágenes... \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/401 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "from UAVIR_tools.gen_movie import genMovie\n",
    "\n",
    "###########################\n",
    "# Seleccionamos el vídeo  #\n",
    "movie_id = 15                                     \n",
    "data_path = TEST_DATA_PATH\n",
    "\n",
    "score_th = 0.2\n",
    "init_frame = 1000\n",
    "end_frame = 1400\n",
    "###########################\n",
    "\n",
    "# Cargamos el dataset\n",
    "dataset = BIRDSAIDataset(data_path, transforms)\n",
    "\n",
    "# Generamos el vídeo ---\n",
    "model_static_quantized.to(torch.device('cpu'))\n",
    "for i in [15]:\n",
    "  genMovie(i, dataset, model_static_quantized, score_th, output_imgs_path=\"tmp2\",\n",
    "           init_frame = init_frame, end_frame = end_frame, device=\"cpu\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comprobamos tiempos de inferencia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargamos el dataset\n",
    "data_path = TEST_DATA_PATH\n",
    "dataset = BIRDSAIDataset(data_path, transforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 23 s\n",
      "Wall time: 14.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "# Modelo sin cuantizar en GPU\n",
    "\n",
    "model.to(device)\n",
    "model_eval = model.eval()\n",
    "\n",
    "for i in range(20):\n",
    "    output = model_eval([dataset[i][0].to(device)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time \n",
    "# Modelo sin cuantizar en CPU\n",
    "\n",
    "model.to(\"cpu\")\n",
    "model_eval = model.eval()\n",
    "\n",
    "for i in range(20):\n",
    "    output = model_eval([dataset[i][0].to(\"cpu\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time \n",
    "# Modelo cuantizado (únicamente soportado en CPU)\n",
    "\n",
    "model_static_quantized.to(\"cpu\")\n",
    "model_eval = model_static_quantized.eval()\n",
    "\n",
    "for i in range(20):\n",
    "    output = model_eval([dataset[i][0].to(\"cpu\")])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
